# -*- coding: utf-8 -*-
"""DeepLearningC1W2Ex.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AiWgMKw8bm2dhH25RlMLfpR5BiOoPEcF
"""

import math

def basic_sigmoid(x):
  return (1/(1+math.exp(-x)))
basic_sigmoid(3)

import numpy as np
x = np.array([1,2,3])
print(np.exp(x))

x+3

def sigmoid_np(x):
  return (1/(1+np.exp(-x)))
sigmoid_np(x)

def sigmoid_derivative(x):
  return sigmoid_np(x)*(1-sigmoid_np(x))
print("sigmoid derivative(x) = "+ str(sigmoid_derivative(x)))

def image2vector(image):
  v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2], 1)
  return v 

image = np.array([[[ 0.67826139,  0.29380381],
        [ 0.90714982,  0.52835647],
        [ 0.4215251 ,  0.45017551]],

       [[ 0.92814219,  0.96677647],
        [ 0.85304703,  0.52351845],
        [ 0.19981397,  0.27417313]],

       [[ 0.60659855,  0.00533165],
        [ 0.10820313,  0.49978937],
        [ 0.34144279,  0.94630077]]])
print("image shape: "+ str(image.shape))
print("image2vector(image) = "+str(image2vector(image)))

def normalizeRows(x):
  x_norm = np.linalg.norm(x, axis = 1, keepdims=True)
  print("x shape: "+str(x.shape))
  print("x_norm shape: "+str(x_norm.shape)) 
  return x/x_norm #broadcasting
  
x = np.array([
    [0, 3, 4],
    [1, 6, 4]])
print("normalizeRows(x): "+str(normalizeRows(x)))

def softmax(x):
  x_exp_vector = np.exp(x)
  x_exp_vector_sum = np.sum(x_exp_vector, axis = 1, keepdims=True)
  s = x_exp_vector/x_exp_vector_sum
  print("x exponent vector shape : "+str(x_exp_vector.shape))
  print("x exponent sum vector shape: "+str(x_exp_vector_sum.shape))
  print("s (result) shape: "+str(s.shape))
  return s


x = np.array([
    [9, 2, 5, 0, 0],
    [7, 5, 0, 0 ,0]])
print("softmax(x) = " + str(softmax(x)))

# classic computation algorithms
import time

x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###
tic = time.process_time()
dot = 0
for i in range(len(x1)):
    dot+= x1[i]*x2[i]
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC OUTER PRODUCT IMPLEMENTATION ###
tic = time.process_time()
outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros
for i in range(len(x1)):
    for j in range(len(x2)):
        outer[i,j] = x1[i]*x2[j]
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC ELEMENTWISE IMPLEMENTATION ###
tic = time.process_time()
mul = np.zeros(len(x1))
for i in range(len(x1)):
    mul[i] = x1[i]*x2[i]
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###
W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array
tic = time.process_time()
gdot = np.zeros(W.shape[0])
for i in range(W.shape[0]):
    for j in range(len(x1)):
        gdot[i] += W[i,j]*x1[j]
toc = time.process_time()
print ("gdot = " + str(gdot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

#using vectorized(numpy) implementations(faster)
x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### VECTORIZED DOT PRODUCT OF VECTORS ###
tic = time.process_time()
dot = np.dot(x1,x2)
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED OUTER PRODUCT ###
tic = time.process_time()
outer = np.outer(x1,x2)
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED ELEMENTWISE MULTIPLICATION ###
tic = time.process_time()
mul = np.multiply(x1,x2)
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED GENERAL DOT PRODUCT ###
tic = time.process_time()
dot = np.dot(W,x1)
toc = time.process_time()
print ("gdot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

def L1(yhat, y):
  return np.sum(np.abs(y-yhat))

yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
print("L1 = " + str(L1(yhat,y)))

#As a reminder, if $x = [X1, X2, ..., Xn]$, 
#then np.dot(x,x) = summation(from 0 to n) (Xj^2).

def L2(yhat, y):
  return np.sum(np.dot((y-yhat),(y-yhat)))

yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
print("L2 = " + str(L2(yhat,y)))